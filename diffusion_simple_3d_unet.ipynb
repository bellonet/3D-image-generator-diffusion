{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b8b6c2-3ca0-4dd9-b67a-b1545a3b2328",
   "metadata": {},
   "source": [
    "In this notebook I adapt a 3D unet meant for segmentation and use it for image generation using a diffusion process.\n",
    "Once reading, will be run on the cluster with varying arguments (grid search).\n",
    "\n",
    "Run:  \n",
    "papermill input_notebook.ipynb output_notebook.ipynb -p arg1 value1 -p arg2 value2\n",
    "\n",
    "3D-UNET from here:  \n",
    "https://github.com/mobarakol/3D_Attention_UNet/tree/main  \n",
    "Works on input. Currently input size is 32x64x64 (batch size 4) - smaller than that doesn't work with this network. Will have to reduce network size..  \n",
    "\n",
    "Paper:  \n",
    "Islam, M., Vibashan, V. S., Jose, V. J. M., Wijethilake, N., Utkarsh, U., & Ren, H. (2020). Brain tumor segmentation and survival prediction using 3D attention UNet. In Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 5th International Workshop, BrainLes 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, Revised Selected Papers, Part I 5 (pp. 262-272). Springer International Publishing.\n",
    "\n",
    "Implementing diffusion from here:  \n",
    "https://github.com/huggingface/diffusion-models-class/blob/main/unit1/01_introduction_to_diffusers.ipynb\n",
    "\n",
    "Current TODO:\n",
    "* Fine tunning\n",
    "* Pick better training images!!\n",
    "* Is set seed possible? (for reproducibility)\n",
    "* Look into model complexity, maybe model is too complex for the data?\n",
    "* Deal with padding - current output has a \"frame\".\n",
    "  \n",
    "Fine-tuning Considerations:\n",
    "* Current attention - spatial+channel - need to ensure that it doesn't overly focus on small details and ignore broader features.\n",
    "* create_conv order parameter - can change the relu type and add normalization. num_groups can be changed too if used. Also, batchnorm seems to be possible either before or after conv (adding b to string).\n",
    "* Activation Order: create_conv can include batch normalization, group normalization and so on - depending on the task, the optimal order of operations might differ.\n",
    "* Regularization - dropouts, weight decay..\n",
    "* Final activation - currently input is normalized [0,1] Probably want sigmoid on the output.\n",
    "* Input size and depth of U-Net.\n",
    "* Fine-tune augmentation.\n",
    "* Early Stopping? Im saving weights every 5 epochs anyway..\n",
    "* Lower learning rate? or learning rate annealing?\n",
    "\n",
    "Comments:\n",
    "* torchsummary is installed via pip with the current github version, not the pip install default version as summary_string is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9cda44-8144-4ced-806b-b8bfa63ed8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "import tifffile as tif\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': '{: 0.2f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed68c1c4-4442-4e63-9906-cc86b595776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "#logging.info(\"Starting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4416dfc7-8aa3-4fc6-97ed-6c77f7e8a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ella/miniconda3/envs/torch_hugging/lib/python3.11/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1682343970094/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e9997-53f1-46b5-b69c-96fb2bea9556",
   "metadata": {},
   "source": [
    "## Set default values for arguments:\n",
    "arguments are set when the notebook is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5d1b5e5-3d5c-4d00-8f54-4c65bb466742",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "arg_input_dir = '32x64x64'\n",
    "arg_batch_size = 4 # looks like 4 might be the maximum. WHY??\n",
    "arg_n_epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f38b52-504e-4dd3-8300-4df1cc1db189",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'{arg_input_dir}_batchSize{arg_batch_size}'\n",
    "output_path = os.path.join('results',output_dir)\n",
    "#os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45960216-87c2-4e6c-9be1-3b21250ebd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_plot = False\n",
    "if not is_plot:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09a3a9-325a-4b3c-9fe2-1d2f846375d5",
   "metadata": {},
   "source": [
    "## Define Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c33499-e4ff-45fb-b52f-3c7bdb4a83e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ella/miniconda3/envs/torch_hugging/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import torchio as tio\n",
    "from torchio import RandomBlur, RandomElasticDeformation, RandomFlip, RescaleIntensity\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8c7976-d21e-4a54-adc2-bac30d2692f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of training images: 684\n"
     ]
    }
   ],
   "source": [
    "image_paths = glob(os.path.join('data', 'training_set_20230816', arg_input_dir, '*.tif'))\n",
    "\n",
    "logging.info(f'Number of training images: {len(image_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8417814-b779-4fe8-a279-afe2a045b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## going with pretty conservative transform values for now:\n",
    "transforms = tio.Compose([\n",
    "    RescaleIntensity(out_min_max=(0, 1)),\n",
    "    RandomElasticDeformation(num_control_points=7, max_displacement=3),\n",
    "    RandomFlip(axes=(0, 1, 2)),\n",
    "    RandomBlur(std=(0.5)) ## (0.5, 1.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d23344-3275-435b-898d-7df0e1541a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "for image_path in image_paths:\n",
    "    subject = tio.Subject(\n",
    "        image=tio.ScalarImage(image_path),\n",
    "    )\n",
    "    subjects.append(subject)\n",
    "\n",
    "dataset = tio.SubjectsDataset(subjects, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71548edc-a562-41e9-be7d-5a7eebaa0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=arg_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b65b8ad0-5622-4272-ab5c-905af7e46146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going through my data once (while introducing transformations)\n",
    "for batch in dataloader:\n",
    "    images = batch['image']\n",
    "    break ## just to do one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d824c108-e32d-499d-8480-4d9b2821f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model input size: torch.Size([4, 1, 64, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "logging.info(f'Model input size: {images[\"data\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bb1ed-ac49-4fdd-8daa-51f7cd25bd3d",
   "metadata": {},
   "source": [
    "## Define curreption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1281d6fb-e868-404e-a96d-40f89ade87ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpy9hfs9sl\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpy9hfs9sl/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97f221bf-eb3e-40f4-b6e7-0294f564cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # going through my data once (while introducing transformations)\n",
    "# for batch in dataloader:\n",
    "#     images = batch['image']\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733abd65-bad3-4646-b1b5-3cb07b61555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(x, amount):\n",
    "    \"\"\"Corrupt the input `x` by mixing it with noise according to `amount`\"\"\"\n",
    "    noise = torch.rand_like(x)\n",
    "    #noise = torch.randn_like(x) * x.std() + x.mean()\n",
    "    amount = amount.view(-1, 1, 1, 1, 1) # Add an extra dimension for 3D data\n",
    "    return x*(1-amount) + noise*amount \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf21d4ef-3ecd-4804-be04-d8b52b55f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding noise\n",
    "amount = torch.linspace(0, 1, images['data'].shape[0]) # Left to right -> more corruption\n",
    "noised_x = corrupt(images['data'], amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55bfd892-c817-4dd1-b209-a25164ab9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_index = 4\n",
    "x_slice = images['data'][:, :, :, :, slice_index]\n",
    "x_noised_slice = noised_x[:,:,:,:,slice_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7f197b-8255-4365-83e5-5733f55dd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_plot:\n",
    "    # Plotting the input data\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 7))\n",
    "    axs[0].set_title('Input data')\n",
    "    axs[0].imshow(torchvision.utils.make_grid(x_slice)[0], cmap='Greys')\n",
    "    \n",
    "    # Plotting the noised version\n",
    "    axs[1].set_title('Corrupted data (-- amount increases -->)')\n",
    "    axs[1].imshow(torchvision.utils.make_grid(x_noised_slice)[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa56e78-2c6a-4191-a97c-6532f9acb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a24f88b-a7b4-4eab-94f9-e41857e46c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = torch.linspace(0, 999, 4).long().to(device)\n",
    "noise = torch.randn_like(images[\"data\"])\n",
    "noisy_x = noise_scheduler.add_noise(images[\"data\"], noise, timesteps)\n",
    "#print(\"Noisy X shape\", noisy_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1c22313-6ea4-41f3-bd16-1ec7115c515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_plot:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,10))\n",
    "    ax[0].imshow(noisy_x[0,0,:,:,slice_index])\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(noisy_x[3,0,:,:,slice_index])\n",
    "    ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769da928-7508-4e58-bf99-827c580b82e1",
   "metadata": {},
   "source": [
    "## Define NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01522485-abd4-4cf5-9b87-9e675bd2adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary, summary_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469d359-ee1a-4549-9d07-6165b20f4a6e",
   "metadata": {},
   "source": [
    "### Building blocks for the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61ffbce7-8689-47b8-aaa7-687e53253214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D version of the SCA (Spatial and Channel Attention) layer - \n",
    "# Its an attention mechanism that takes into account both spatial and channel-wise relationships within the input data\n",
    "class SCA3D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel // reduction)),\n",
    "                                                nn.ReLU(inplace=True),\n",
    "                                                nn.Linear(int(channel // reduction), channel))\n",
    "        self.spatial_se = nn.Conv3d(channel, 1, kernel_size=1,\n",
    "                                    stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _, _ = x.size() ## bahs=batch_size\n",
    "        chn_se = self.avg_pool(x).view(bahs, chs)\n",
    "        chn_se = torch.sigmoid(self.channel_excitation(chn_se).view(bahs, chs, 1, 1,1))\n",
    "        chn_se = torch.mul(x, chn_se)\n",
    "        spa_se = torch.sigmoid(self.spatial_se(x))\n",
    "        spa_se = torch.mul(x, spa_se)\n",
    "        net_out = spa_se + x + chn_se\n",
    "        return net_out\n",
    "\n",
    "\n",
    "def conv3d(in_channels, out_channels, kernel_size, bias, padding=1):\n",
    "    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=1):\n",
    "    \"\"\"\n",
    "    Create a list of modules that together constitute a single conv layer with non-linearity\n",
    "    and optional batchnorm/groupnorm.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        order (string): order of things, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int): add zero-padding to the input\n",
    "    Return:\n",
    "        list of tuple (name, module)\n",
    "    \"\"\"\n",
    "    assert 'c' in order, \"Conv layer MUST be present\"\n",
    "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
    "\n",
    "    modules = []\n",
    "    for i, char in enumerate(order):\n",
    "        if char == 'r':\n",
    "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
    "        elif char == 'l':\n",
    "            modules.append(('LeakyReLU', nn.LeakyReLU(negative_slope=0.1, inplace=True)))\n",
    "        elif char == 'e':\n",
    "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
    "        elif char == 'c':\n",
    "            # add learnable bias only in the absence of gatchnorm/groupnorm\n",
    "            bias = not ('g' in order or 'b' in order)\n",
    "            modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
    "        elif char == 'g':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            assert not is_before_conv, 'GroupNorm MUST go after the Conv3d'\n",
    "            # number of groups must be less or equal the number of channels\n",
    "            if out_channels < num_groups:\n",
    "                num_groups = out_channels\n",
    "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)))\n",
    "        elif char == 'b':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
    "            else:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c']\")\n",
    "\n",
    "    return modules\n",
    "\n",
    "\n",
    "class SingleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
    "    of operations can be specified via the `order` parameter\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size (int): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='crg', num_groups=8, padding=1):\n",
    "        super(SingleConv, self).__init__()\n",
    "\n",
    "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n",
    "            self.add_module(name, module)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
    "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
    "    This can be changed however by providing the 'order' argument, e.g. in order\n",
    "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
    "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
    "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
    "        kernel_size (int): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='crg', num_groups=8):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        if encoder:\n",
    "            # we're in the encoder path\n",
    "            conv1_in_channels = in_channels\n",
    "            conv1_out_channels = out_channels // 2\n",
    "            if conv1_out_channels < in_channels:\n",
    "                conv1_out_channels = in_channels\n",
    "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
    "        else:\n",
    "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
    "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
    "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
    "\n",
    "        # conv1\n",
    "        self.add_module('SingleConv1',\n",
    "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups))\n",
    "        # conv2\n",
    "        self.add_module('SingleConv2',\n",
    "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups))\n",
    "\n",
    "\n",
    "class ExtResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic UNet block consisting of a SingleConv followed by the residual block.\n",
    "    The SingleConv takes care of increasing/decreasing the number of channels and also ensures that the number\n",
    "    of output channels is compatible with the residual block that follows.\n",
    "    This block can be used instead of standard DoubleConv in the Encoder module.\n",
    "    Motivated by: https://arxiv.org/pdf/1706.00120.pdf\n",
    "    Notice we use ELU instead of ReLU (order='cge') and put non-linearity after the groupnorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cge', num_groups=8, **kwargs):\n",
    "        super(ExtResNetBlock, self).__init__()\n",
    "\n",
    "        # first convolution\n",
    "        self.conv1 = SingleConv(in_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
    "        # residual block\n",
    "        self.conv2 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
    "        # remove non-linearity from the 3rd convolution since it's going to be applied after adding the residual\n",
    "        n_order = order\n",
    "        for c in 'rel':\n",
    "            n_order = n_order.replace(c, '')\n",
    "        self.conv3 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=n_order,\n",
    "                                num_groups=num_groups)\n",
    "\n",
    "        # create non-linearity separately\n",
    "        if 'l' in order:\n",
    "            self.non_linearity = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        elif 'e' in order:\n",
    "            self.non_linearity = nn.ELU(inplace=True)\n",
    "        else:\n",
    "            self.non_linearity = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply first convolution and save the output as a residual\n",
    "        out = self.conv1(x)\n",
    "        residual = out\n",
    "\n",
    "        # residual block\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.non_linearity(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module from the encoder path consisting of the optional max\n",
    "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
    "    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
    "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
    "    a DoubleConv module.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int): size of the convolving kernel\n",
    "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
    "        pool_kernel_size (tuple): the size of the window to take a max over\n",
    "        pool_type (str): pooling layer: 'max' or 'avg'\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
    "                 pool_kernel_size=(2, 2, 2), pool_type='max', basic_module=DoubleConv, conv_layer_order='crg',\n",
    "                 num_groups=8):\n",
    "        super(Encoder, self).__init__()\n",
    "        assert pool_type in ['max', 'avg']\n",
    "        if apply_pooling:\n",
    "            if pool_type == 'max':\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
    "            else:\n",
    "                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
    "        else:\n",
    "            self.pooling = None\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=True,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pooling is not None:\n",
    "            x = self.pooling(x)\n",
    "        #x = self.scse(x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module for decoder path consisting of the upsample layer\n",
    "    (either learned ConvTranspose3d or interpolation) followed by a DoubleConv\n",
    "    module.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size (int): size of the convolving kernel\n",
    "        scale_factor (tuple): used as the multiplier for the image H/W/D in\n",
    "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
    "            from the corresponding encoder\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
    "                 scale_factor=(2, 2, 2), basic_module=DoubleConv, conv_layer_order='crg', num_groups=8):\n",
    "        super(Decoder, self).__init__()\n",
    "        if basic_module == DoubleConv:\n",
    "            # if DoubleConv is the basic_module use nearest neighbor interpolation for upsampling\n",
    "            self.upsample = None\n",
    "        else:\n",
    "            # otherwise use ConvTranspose3d (bear in mind your GPU memory)\n",
    "            # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
    "            # (D_out = (D_in − 1) ×  stride[0] − 2 ×  padding[0] +  kernel_size[0] +  output_padding[0])\n",
    "            # also scale the number of channels from in_channels to out_channels so that summation joining\n",
    "            # works correctly\n",
    "            self.upsample = nn.ConvTranspose3d(in_channels,\n",
    "                                               out_channels,\n",
    "                                               kernel_size=kernel_size,\n",
    "                                               stride=scale_factor,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1)\n",
    "            # adapt the number of in_channels for the ExtResNetBlock\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.scse = SCA3D(in_channels)\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=False,\n",
    "                                         kernel_size=kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups)\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        if self.upsample is None:\n",
    "            # use nearest neighbor interpolation and concatenation joining\n",
    "            output_size = encoder_features.size()[2:]\n",
    "            x = F.interpolate(x, size=output_size, mode='nearest')\n",
    "            # concatenate encoder_features (encoder path) with the upsampled input across channel dimension\n",
    "            x = torch.cat((encoder_features, x), dim=1)\n",
    "        else:\n",
    "            # use ConvTranspose3d and summation joining\n",
    "            x = self.upsample(x)\n",
    "            x += encoder_features\n",
    "        x = self.scse(x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A module consisting of a convolution layer (e.g. Conv3d+ReLU+GroupNorm3d) and the final 1x1 convolution\n",
    "    which reduces the number of channels to 'out_channels'.\n",
    "    with the number of output channels 'out_channels // 2' and 'out_channels' respectively.\n",
    "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
    "    This can be change however by providing the 'order' argument, e.g. in order\n",
    "    to change to Conv3d+BatchNorm3d+ReLU use order='cbr'.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size (int): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='crg', num_groups=8):\n",
    "        super(FinalConv, self).__init__()\n",
    "\n",
    "        # conv1\n",
    "        self.add_module('SingleConv', SingleConv(in_channels, in_channels, kernel_size, order, num_groups))\n",
    "\n",
    "        # in the last layer a 1×1 convolution reduces the number of output channels to out_channels\n",
    "        final_conv = nn.Conv3d(in_channels, out_channels, 1)\n",
    "        self.add_module('final_conv', final_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af95ae1-e498-40b8-a6d9-8e4e63fdcafc",
   "metadata": {},
   "source": [
    "### Define unet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beff9ffb-602b-4357-8640-21c9fbdd9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_maps(init_channel_number, number_of_fmaps):\n",
    "    return [init_channel_number * 2 ** k for k in range(number_of_fmaps)]\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3DUnet model from\n",
    "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
    "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output segmentation masks;\n",
    "            Note that that the of out_channels might correspond to either\n",
    "            different semantic classes or to different binary segmentation mask.\n",
    "            It's up to the user of the class to interpret the out_channels and\n",
    "            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n",
    "            or BCEWithLogitsLoss (two-class) respectively)\n",
    "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
    "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
    "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n",
    "            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n",
    "            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n",
    "        layer_order (string): determines the order of layers\n",
    "            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.\n",
    "            See `SingleConv` for more info\n",
    "        init_channel_number (int): number of feature maps in the first conv layer of the encoder; default: 64\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid, f_maps=16, layer_order='crg', num_groups=8,\n",
    "                 **kwargs):\n",
    "        super(UNet3D, self).__init__()\n",
    "\n",
    "        if isinstance(f_maps, int):\n",
    "            # use 4 levels in the encoder path as suggested in the paper\n",
    "            f_maps = create_feature_maps(f_maps, number_of_fmaps=6)\n",
    "\n",
    "        # create encoder path consisting of Encoder modules. The length of the encoder is equal to `len(f_maps)`\n",
    "        # uses DoubleConv as a basic_module for the Encoder\n",
    "        encoders = []\n",
    "        for i, out_feature_num in enumerate(f_maps):\n",
    "            if i == 0:\n",
    "                encoder = Encoder(in_channels, out_feature_num, apply_pooling=False, basic_module=DoubleConv,\n",
    "                                  conv_layer_order=layer_order, num_groups=num_groups)\n",
    "            else:\n",
    "                encoder = Encoder(f_maps[i - 1], out_feature_num, basic_module=DoubleConv,\n",
    "                                  conv_layer_order=layer_order, num_groups=num_groups)\n",
    "            encoders.append(encoder)\n",
    "\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "\n",
    "        # create decoder path consisting of the Decoder modules. The length of the decoder is equal to `len(f_maps) - 1`\n",
    "        # uses DoubleConv as a basic_module for the Decoder\n",
    "        decoders = []\n",
    "        reversed_f_maps = list(reversed(f_maps))\n",
    "        for i in range(len(reversed_f_maps) - 1):\n",
    "            in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
    "            out_feature_num = reversed_f_maps[i + 1]\n",
    "            decoder = Decoder(in_feature_num, out_feature_num, basic_module=DoubleConv,\n",
    "                              conv_layer_order=layer_order, num_groups=num_groups)\n",
    "            decoders.append(decoder)\n",
    "\n",
    "        self.decoders = nn.ModuleList(decoders)\n",
    "\n",
    "        # in the last layer a 1×1 convolution reduces the number of output\n",
    "        # channels to the number of labels\n",
    "        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "\n",
    "        if final_sigmoid:\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder part\n",
    "        encoders_features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            # reverse the encoder outputs to be aligned with the decoder\n",
    "            encoders_features.insert(0, x)\n",
    "\n",
    "        # remove the last encoder's output from the list\n",
    "        # !!remember: it's the 1st in the list\n",
    "        pool_fea = self.avg_pool(encoders_features[0]).squeeze(0).squeeze(1).squeeze(1).squeeze(1)\n",
    "        encoders_features = encoders_features[1:]\n",
    "        # decoder part\n",
    "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
    "            # pass the output from the corresponding encoder and the output\n",
    "            # of the previous decoder\n",
    "            x = decoder(encoder_features, x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # apply final_activation (i.e. Sigmoid or Softmax) only for prediction. During training the network outputs\n",
    "        # logits and it's up to the user to normalize it before visualising with tensorboard or computing validation metric\n",
    "        if not self.training:\n",
    "            x = self.final_activation(x)\n",
    "\n",
    "        return x, pool_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bcff15-4245-46cf-b067-5d2c56cb7df2",
   "metadata": {},
   "source": [
    "## Test the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c97e291d-8f31-4890-a7f9-cbf66cd9025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ella/miniconda3/envs/torch_hugging/lib/python3.11/site-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "INFO:root:Input shape: torch.Size([4, 2, 64, 64, 32])\n",
      "INFO:root:Output shape: torch.Size([4, 1, 64, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "def test_unet(model, dataloader):\n",
    "    for batch in dataloader:\n",
    "        images = batch['image']['data']  # Extracting the data tensor\n",
    "        bs, C, H, W, D = images.shape\n",
    "        #images = images.to(device)  # Moving to the proper device if necessary (e.g., GPU)\n",
    "\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_train_timesteps, (bs,), device=images.device\n",
    "        ).long()\n",
    "\n",
    "        # Expand timesteps to have the same spatial dimensions as images\n",
    "        timesteps = timesteps.view(bs, 1, 1, 1, 1)\n",
    "        timesteps = timesteps.repeat(1, 1, H, W, D).float()\n",
    "\n",
    "        images_with_timesteps = torch.cat([images, timesteps], dim=1)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        output, _ = model(images_with_timesteps.to(device))\n",
    "\n",
    "        logging.info(f'Input shape: {images_with_timesteps.shape}')\n",
    "        logging.info(f'Output shape: {output.shape}')\n",
    "        break  # Stop after the first batch\n",
    "\n",
    "# Initialize the UNet3D model with 1 input channel and 1 output channel\n",
    "unet_model = UNet3D(in_channels=2, out_channels=1, final_sigmoid=True).to(device)\n",
    "\n",
    "# Test the UNet model\n",
    "test_unet(unet_model, dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37be79a8-8a16-4bf4-b175-058120f582e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 64 64 32\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    images = batch['image']['data']\n",
    "    bs, C, H, W, D = images.shape\n",
    "    break ## just to do one iteration\n",
    "\n",
    "print(C, H, W, D)\n",
    "    \n",
    "#summary(unet_model, (2, H, W, D)) ## print \n",
    "model_summary_str, _ = summary_string(unet_model, \n",
    "                                      (2, H, W, D), \n",
    "                                      batch_size=-1, \n",
    "                                      device=device)#torch.device('cuda:0')\n",
    "\n",
    "with open(os.path.join(output_path,'model_summary.txt'), 'w') as f:\n",
    "    f.write(model_summary_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050e884-1388-40b1-9463-df5dc9a29e4e",
   "metadata": {},
   "source": [
    "## Create a Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871c604-6683-4c19-a0f3-0460a14d0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the noise scheduler\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\"\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(unet_model.parameters(), lr=4e-4)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(arg_n_epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "    \n",
    "        clean_images = batch[\"image\"]['data'].to(device)\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs, C, H, W, D = clean_images.shape\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        \n",
    "        timesteps = timesteps.view(bs, 1, 1, 1, 1)\n",
    "        timesteps = timesteps.repeat(1, 1, H, W, D).float()\n",
    "\n",
    "        noisy_images_with_timesteps = torch.cat([noisy_images, timesteps], dim=1)\n",
    "\n",
    "        # Get the model prediction\n",
    "        noise_pred, _ = unet_model(noisy_images_with_timesteps)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update the model parameters with the optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        loss_last_epoch = sum(losses[-len(dataloader) :]) / len(dataloader)\n",
    "        logging.info(f'Epoch:{epoch+1}, loss: {loss_last_epoch}')\n",
    "\n",
    "        torch.save(unet_model.state_dict(), \n",
    "                   os.path.join(output_path, f'model_weights_epoch_{epoch+1}.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff0755-a2ba-40b7-9dea-92f318f6c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(dataloader)\n",
    "\n",
    "with open(os.path.join(output_path,'losses.txt'), 'w') as f:\n",
    "    for i,l in enumerate(losses):\n",
    "        if i%n_batches==0:\n",
    "            f.write(f'EPOCH {i//n_batches}\\n')\n",
    "        f.write(f'{str(l)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7a552-8c61-4b34-a184-7bb533951bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].plot(losses)\n",
    "axs[1].plot(np.log(losses))\n",
    "\n",
    "if is_plot:   \n",
    "    plt.show()\n",
    "else:\n",
    "    plt.savefig(os.path.join(output_path,'losses.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086082a-7e97-44d4-a0ab-74b68f403b8c",
   "metadata": {},
   "source": [
    "## Generate images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244f026-8a9a-4712-9587-a16363e09e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random starting point (8 random images):\n",
    "sample = torch.randn(bs, C, H, W, D).to(device)\n",
    "\n",
    "for i, t in enumerate(noise_scheduler.timesteps):\n",
    "\n",
    "    # Extend timesteps similar to what's done in the training loop\n",
    "    timesteps = torch.full((bs, 1, H, W, D), t, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Concatenate sample with timestep\n",
    "    sample_with_timesteps = torch.cat([sample, timesteps], dim=1)\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual, _ = unet_model(sample_with_timesteps)\n",
    "\n",
    "    # Update sample with step\n",
    "    sample = noise_scheduler.step(residual, t, sample).prev_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce3b71-e166-4380-92d0-ab627f5935fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_plot:\n",
    "    slice_index = 4\n",
    "    x_slice = sample[:, :, :, :, slice_index].cpu()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(12, 7))\n",
    "    axs.set_title('Gerentaed Images')\n",
    "    axs.imshow(torchvision.utils.make_grid(x_slice)[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8757e3-bcca-4796-8dc6-7d2fdc55e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_np = sample.cpu().numpy()\n",
    "\n",
    "# Save each image in the batch\n",
    "for idx, img in enumerate(sample_np):\n",
    "    img_t = np.transpose(img[0], (2, 0, 1))\n",
    "    tif.imsave(os.path.join(output_path, f'image_{idx}.tif'), img_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
